{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oL0J30nkWwXQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "oL0J30nkWwXQ",
        "outputId": "bea5129b-f346-4067-ed6d-0398185f9a4e"
      },
      "outputs": [],
      "source": [
        "!pip install lpips\n",
        "!pip install torchmetrics\n",
        "!pip install colormath\n",
        "!pip install torch_fidelity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "irPTmsnEXzXE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irPTmsnEXzXE",
        "outputId": "58d94013-7f52-4029-8064-0f1eab2370cf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(\"CUDA Available:\", torch.cuda.is_available())\n",
        "print(\"Device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
        "print(\"Current device:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2256fbd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2256fbd",
        "outputId": "fa7652fb-0615-4d11-83eb-fb8c779798d5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import gc\n",
        "import math\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.utils import make_grid,save_image\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "from google.colab import files\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import torch_fidelity\n",
        "from torch_fidelity import calculate_metrics\n",
        "# LPIPS, FID, and color metrics\n",
        "import lpips\n",
        "from torchmetrics.image.fid import FrechetInceptionDistance\n",
        "from colormath.color_diff import delta_e_cie2000\n",
        "from colormath.color_objects import LabColor, sRGBColor\n",
        "\n",
        "\n",
        "# CUDA configuration\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# Device setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad0961e8",
      "metadata": {
        "id": "ad0961e8"
      },
      "outputs": [],
      "source": [
        "class NoiseScheduler:\n",
        "    def __init__(self, num_timesteps=1000, beta_start=1e-4, beta_end=2e-2):\n",
        "        self.num_timesteps = num_timesteps\n",
        "        self.betas = torch.linspace(beta_start, beta_end, num_timesteps)\n",
        "        self.alphas = 1. - self.betas\n",
        "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
        "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
        "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - self.alphas_cumprod)\n",
        "\n",
        "    def add_noise(self, original, noise, timesteps):\n",
        "        # Remove .to(original.device) since tensors are already on device\n",
        "        sqrt_alpha_prod = self.sqrt_alphas_cumprod[timesteps]\n",
        "        sqrt_one_minus_alpha_prod = self.sqrt_one_minus_alphas_cumprod[timesteps]\n",
        "\n",
        "        # Expand dimensions for broadcasting\n",
        "        sqrt_alpha_prod = sqrt_alpha_prod[:, None, None, None]\n",
        "        sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod[:, None, None, None]\n",
        "\n",
        "        return sqrt_alpha_prod * original + sqrt_one_minus_alpha_prod * noise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f47720ad",
      "metadata": {
        "id": "f47720ad"
      },
      "outputs": [],
      "source": [
        "class CosineNoiseScheduler(NoiseScheduler):\n",
        "    def __init__(self, num_timesteps=1000, s=0.008, device='cuda'):\n",
        "        super().__init__(num_timesteps)\n",
        "        self.s = s\n",
        "        self.device = device\n",
        "        self._build_schedule()\n",
        "\n",
        "    def _build_schedule(self):\n",
        "        steps = torch.arange(self.num_timesteps + 1, device=self.device)\n",
        "        f_t = torch.cos((steps / self.num_timesteps + self.s) / (1 + self.s) * math.pi * 0.5) ** 2\n",
        "        self.alphas_cumprod = f_t / f_t[0]\n",
        "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
        "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - self.alphas_cumprod)\n",
        "\n",
        "    def remove_noise(self, x_t, predicted_noise, timesteps):\n",
        "        \"\"\"Reverse diffusion process to estimate x_0 from x_t\"\"\"\n",
        "        sqrt_alpha_prod = self.sqrt_alphas_cumprod[timesteps][:, None, None, None]\n",
        "        sqrt_one_minus_alpha_prod = self.sqrt_one_minus_alphas_cumprod[timesteps][:, None, None, None]\n",
        "\n",
        "        x_0 = (x_t - sqrt_one_minus_alpha_prod * predicted_noise) / sqrt_alpha_prod\n",
        "        return x_0.clamp(-1, 1)  # Ensure valid pixel range\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6a23818",
      "metadata": {
        "id": "c6a23818"
      },
      "outputs": [],
      "source": [
        "class TimestepEmbedding(nn.Module):\n",
        "    def __init__(self, dim=64):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Linear(dim, dim * 4),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(dim * 4, dim * 4)\n",
        "        )\n",
        "\n",
        "    def forward(self, t):\n",
        "        half_dim = self.dim // 2\n",
        "        emb = math.log(10000) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
        "        emb = t.float()[:, None] * emb[None, :]\n",
        "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
        "        return self.proj(emb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e402183",
      "metadata": {
        "id": "2e402183"
      },
      "outputs": [],
      "source": [
        "class TimestepResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, time_embed_dim):\n",
        "        super().__init__()\n",
        "        self.time_proj = nn.Linear(time_embed_dim, out_channels)  # (256 -> out_channels)\n",
        "        self.block = nn.Sequential(\n",
        "            nn.GroupNorm(1, in_channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "            nn.GroupNorm(1, out_channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
        "        )\n",
        "        self.time_proj = nn.Linear(time_embed_dim, out_channels)\n",
        "        self.skip = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x, temb = inputs\n",
        "        h = self.block(x)\n",
        "        h = h + self.time_proj(temb)[:, :, None, None]\n",
        "        return h + self.skip(x), temb  # Pass through timestep embedding\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fba1f3c",
      "metadata": {
        "id": "7fba1f3c"
      },
      "outputs": [],
      "source": [
        "# class ConditionalUNet(nn.Module):\n",
        "#     def __init__(self, in_channels=4, base_channels=64, time_embed_dim=64):\n",
        "#         super().__init__()\n",
        "#         self.time_embed = TimestepEmbedding(time_embed_dim)\n",
        "\n",
        "#         self.down1 = nn.Sequential(\n",
        "#             TimestepResidualBlock(in_channels, base_channels, time_embed_dim),\n",
        "#             TimestepResidualBlock(base_channels, base_channels, time_embed_dim)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x, cond, t):\n",
        "#         temb = self.time_embed(t)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd89135e",
      "metadata": {
        "id": "bd89135e"
      },
      "outputs": [],
      "source": [
        "class TimestepUNet(nn.Module):\n",
        "    def __init__(self, in_channels=4, base_channels=64, time_dim=64):\n",
        "        super().__init__()\n",
        "        self.time_dim = time_dim\n",
        "        self.time_embed = nn.Sequential(\n",
        "            nn.Linear(time_dim, time_dim * 4),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(time_dim * 4, time_dim * 4)\n",
        "        )\n",
        "\n",
        "        # Downsample blocks\n",
        "        self.down1 = nn.ModuleList([\n",
        "            TimestepResidualBlock(in_channels, base_channels, time_dim*4),\n",
        "            TimestepResidualBlock(base_channels, base_channels, time_dim*4)\n",
        "        ])\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.down2 = nn.ModuleList([\n",
        "            TimestepResidualBlock(base_channels, base_channels*2, time_dim*4),\n",
        "            TimestepResidualBlock(base_channels*2, base_channels*2, time_dim*4)\n",
        "        ])\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "\n",
        "        # Middle blocks\n",
        "        self.mid = nn.ModuleList([\n",
        "            TimestepResidualBlock(base_channels*2, base_channels*4, time_dim*4),\n",
        "            TimestepResidualBlock(base_channels*4, base_channels*4, time_dim*4)\n",
        "        ])\n",
        "\n",
        "        # Upsample blocks\n",
        "        self.up2 = nn.ModuleList([\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            TimestepResidualBlock(base_channels*4 + base_channels*2, base_channels*2, time_dim*4)\n",
        "        ])\n",
        "\n",
        "        self.up1 = nn.ModuleList([\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            TimestepResidualBlock(base_channels*2 + base_channels, base_channels, time_dim*4)\n",
        "        ])\n",
        "\n",
        "        self.out = nn.Conv2d(base_channels, 3, kernel_size=1)\n",
        "\n",
        "    def forward(self, x, cond, t):\n",
        "        temb = self.time_embed(get_timestep_embedding(t, self.time_dim))\n",
        "        x = torch.cat([x, cond], dim=1)\n",
        "\n",
        "        # Downsample path\n",
        "        # Down1 blocks\n",
        "        for block in self.down1:\n",
        "            x, _ = block((x, temb))\n",
        "        d1 = x  # Save before pooling\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        # Down2 blocks\n",
        "        for block in self.down2:\n",
        "            x, _ = block((x, temb))\n",
        "        d2 = x  # Save before pooling\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        # Middle path\n",
        "        for block in self.mid:\n",
        "            x, _ = block((x, temb))\n",
        "\n",
        "        # Upsample path\n",
        "        # Up2 blocks\n",
        "        x = self.up2[0](x)\n",
        "        x = torch.cat([x, d2], dim=1)  # Use pre-pool d2 (96 channels)\n",
        "        for block in self.up2[1:]:\n",
        "            x, _ = block((x, temb))\n",
        "\n",
        "        # Up1 blocks\n",
        "        x = self.up1[0](x)\n",
        "        x = torch.cat([x, d1], dim=1)  # Use pre-pool d1 (48 channels)\n",
        "        for block in self.up1[1:]:\n",
        "            x, _ = block((x, temb))\n",
        "\n",
        "        return self.out(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "735b7fe9",
      "metadata": {
        "id": "735b7fe9"
      },
      "outputs": [],
      "source": [
        "class CIFAR10ColorGrayUpscaled(Dataset):\n",
        "    def __init__(self, root='./data', train=True, upscale_size=128):\n",
        "        self.upscale_size = upscale_size\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((upscale_size, upscale_size)),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "        self.gray_transform = transforms.Grayscale(num_output_channels=1)\n",
        "\n",
        "        self.dataset = datasets.CIFAR10(\n",
        "            root=root,\n",
        "            train=train,\n",
        "            download=True,\n",
        "            transform=self.transform\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        color_img, _ = self.dataset[idx]\n",
        "        pil_image = Image.fromarray(self.dataset.data[idx])\n",
        "        gray_img = self.transform(self.gray_transform(pil_image))\n",
        "\n",
        "        return {'color': color_img, 'gray': gray_img}\n",
        "\n",
        "# Memory-Optimized DataLoader\n",
        "def get_cifar10_loader(batch_size=64, upscale_size=128, train=True):\n",
        "    dataset = CIFAR10ColorGrayUpscaled(train=train, upscale_size=upscale_size)\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=train,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        persistent_workers=True\n",
        "    )\n",
        "\n",
        "\n",
        "class CombinedLoss(nn.Module):\n",
        "    def __init__(self, lpips_weight=1.0, deltae_weight=1.0, mse_weight=1.0, device='cuda'):\n",
        "        super().__init__()\n",
        "        self.lpips_fn = lpips.LPIPS(net='vgg').to(device)\n",
        "        self.lpips_weight = lpips_weight\n",
        "        self.deltae_weight = deltae_weight\n",
        "        self.mse_weight = mse_weight\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        # Assumes input shape is (B, 3, H, W), normalized [-1, 1] for LPIPS\n",
        "        loss = 0.0\n",
        "\n",
        "        # MSE\n",
        "        if self.mse_weight > 0:\n",
        "            mse_loss = F.mse_loss(pred, target)\n",
        "            loss += self.mse_weight * mse_loss\n",
        "\n",
        "        # LPIPS\n",
        "        if self.lpips_weight > 0:\n",
        "            lpips_val = self.lpips_fn(pred, target).mean()\n",
        "            loss += self.lpips_weight * lpips_val\n",
        "\n",
        "        # ΔE using LAB color difference\n",
        "        if self.deltae_weight > 0:\n",
        "            delta_e_loss = self.batch_delta_e_loss(pred, target)\n",
        "            loss += self.deltae_weight * delta_e_loss\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def batch_delta_e_loss(self, pred, target):\n",
        "        \"\"\"\n",
        "        pred and target: (B, 3, H, W) in range [0, 1] for ΔE\n",
        "        \"\"\"\n",
        "        pred_img = (pred.clamp(0, 1).detach().cpu().permute(0, 2, 3, 1).numpy())  # B,H,W,3\n",
        "        target_img = (target.clamp(0, 1).detach().cpu().permute(0, 2, 3, 1).numpy())\n",
        "\n",
        "        batch_size = pred_img.shape[0]\n",
        "        delta_e_total = 0.0\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            delta_e_img = self.compute_delta_e(pred_img[b], target_img[b])\n",
        "            delta_e_total += np.mean(delta_e_img)\n",
        "\n",
        "        return torch.tensor(delta_e_total / batch_size, device=pred.device)\n",
        "\n",
        "    def compute_delta_e(self, pred_img, target_img):\n",
        "        # Ensure inputs are torch tensors before converting\n",
        "        if isinstance(pred_img, torch.Tensor):\n",
        "            pred_np = pred_img.permute(1, 2, 0).detach().cpu().numpy()  # [H, W, 3]\n",
        "        else:\n",
        "            pred_np = pred_img\n",
        "\n",
        "        if isinstance(target_img, torch.Tensor):\n",
        "            target_np = target_img.permute(1, 2, 0).detach().cpu().numpy()\n",
        "        else:\n",
        "            target_np = target_img\n",
        "\n",
        "        pred_np = np.clip(pred_np, 0.0, 1.0)\n",
        "        target_np = np.clip(target_np, 0.0, 1.0)\n",
        "\n",
        "        pred_uint8 = (pred_np * 255).astype(np.uint8)\n",
        "        target_uint8 = (target_np * 255).astype(np.uint8)\n",
        "\n",
        "        pred_lab = cv2.cvtColor(pred_uint8, cv2.COLOR_RGB2LAB).astype(np.float32)\n",
        "        target_lab = cv2.cvtColor(target_uint8, cv2.COLOR_RGB2LAB).astype(np.float32)\n",
        "\n",
        "        delta_e = np.linalg.norm(pred_lab - target_lab, axis=-1)  # [H, W]\n",
        "        return delta_e\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc79cfa0",
      "metadata": {
        "id": "bc79cfa0"
      },
      "outputs": [],
      "source": [
        "def train_diffusion_model(model, noise_scheduler, criterion, optimizer, num_epochs, device,accumulation_steps=4):\n",
        "    # Initialize training components\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    # Data loaders (optimized for low VRAM)\n",
        "    train_loader = get_cifar10_loader(batch_size=64)  # Reduced batch size\n",
        "    val_loader = get_cifar10_loader(batch_size=64, train=False)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        sys.stdout.write(f\"\\nStarting epoch {epoch+1}\\n\")\n",
        "        sys.stdout.write(f\"Training batches: {len(train_loader)}\\n\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_psnr = 0.0\n",
        "        running_ssim = 0.0\n",
        "        running_lpips = 0.0\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            gray = batch['gray'].to(device, non_blocking=True)\n",
        "            color = batch['color'].to(device, non_blocking=True)\n",
        "\n",
        "            if i % 1000 == 0:\n",
        "                print_gpu_mem()\n",
        "\n",
        "            # Diffusion process\n",
        "            t = torch.randint(0, noise_scheduler.num_timesteps, (color.size(0),), device=device)\n",
        "            noise = torch.randn_like(color)\n",
        "            noised_color = noise_scheduler.add_noise(color, noise, t)\n",
        "\n",
        "            with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
        "                pred_noise = model(noised_color, gray, t)\n",
        "                loss = criterion(pred_noise, noise) / accumulation_steps\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # Gradient accumulation\n",
        "            if (i + 1) % accumulation_steps == 0:\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            # Update running metrics\n",
        "            running_loss += loss.item() * accumulation_steps\n",
        "\n",
        "            # Calculate metrics on denoised version\n",
        "            with torch.no_grad():\n",
        "                denoised = noise_scheduler.remove_noise(noised_color, pred_noise, t)\n",
        "                batch_psnr, batch_ssim = compute_metrics(denoised, color)\n",
        "                batch_lpips = lpips_fn(denoised, color).mean().item()\n",
        "\n",
        "            running_psnr += batch_psnr\n",
        "            running_ssim += batch_ssim\n",
        "            running_lpips += batch_lpips\n",
        "\n",
        "            # Progress update\n",
        "            sys.stdout.write(\n",
        "                f\"\\r[Epoch {epoch+1}/{num_epochs}] [Batch {i+1}/{len(train_loader)}] \"\n",
        "                f\"Loss: {loss.item() * accumulation_steps:.4f} PSNR: {batch_psnr:.2f} \"\n",
        "                f\"SSIM: {batch_ssim:.4f} LPIPS: {batch_lpips:.4f}\"\n",
        "            )\n",
        "            sys.stdout.flush()\n",
        "\n",
        "            if (i + 1) % 100 == 0:\n",
        "                idx = random.randint(0, gray.size(0) - 1)\n",
        "                show_images(gray, denoised, color, idx)\n",
        "\n",
        "        # Epoch statistics\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        avg_psnr = running_psnr / len(train_loader)\n",
        "        avg_ssim = running_ssim / len(train_loader)\n",
        "        avg_lpips = running_lpips / len(train_loader)\n",
        "\n",
        "        sys.stdout.write('\\n')\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for i, batch in enumerate(val_loader):\n",
        "                gray = batch['gray'].to(device)\n",
        "                color = batch['color'].to(device)\n",
        "\n",
        "\n",
        "                t = torch.full((color.size(0),), noise_scheduler.num_timesteps//2, device=device)\n",
        "                noise = torch.randn_like(color)\n",
        "                noised_color = noise_scheduler.add_noise(color, noise, t)\n",
        "\n",
        "                pred_noise = model(noised_color, gray, t)\n",
        "                val_loss += criterion(pred_noise, noise).item()\n",
        "\n",
        "                if i == 0:\n",
        "                    denoised = noise_scheduler.remove_noise(noised_color, pred_noise, t)\n",
        "                    show_images(gray, denoised, color, idx=0)\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        print(f\"[Epoch {epoch+1}] Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            save_checkpoint(model, optimizer, epoch, path=\"best_model.pth\")\n",
        "            sys.stdout.write(\"Best model saved!\\n\")\n",
        "\n",
        "        torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb2f2624",
      "metadata": {
        "id": "fb2f2624"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(model, optimizer, epoch, path):\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }, path)\n",
        "\n",
        "def compute_metrics(pred, target):\n",
        "    # PSNR\n",
        "    mse = torch.mean((pred - target) ** 2)\n",
        "    psnr = 10 * torch.log10(1.0 / mse)\n",
        "\n",
        "    # SSIM\n",
        "    ssim = torchmetrics.functional.structural_similarity_index_measure(\n",
        "        pred, target, data_range=1.0\n",
        "    )\n",
        "    return psnr.item(), ssim.item()\n",
        "\n",
        "def show_images(gray, pred, color, idx=0):\n",
        "    # Convert tensors to numpy arrays\n",
        "    gray_img = gray[idx].cpu().permute(1, 2, 0).numpy()\n",
        "    pred_img = pred[idx].detach().cpu().permute(1, 2, 0).numpy()\n",
        "    color_img = color[idx].cpu().permute(1, 2, 0).numpy()\n",
        "\n",
        "    # Plot images\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    ax[0].imshow(gray_img.squeeze(), cmap='gray')\n",
        "    ax[0].set_title('Input (Gray)')\n",
        "    ax[1].imshow(pred_img)\n",
        "    ax[1].set_title('Predicted')\n",
        "    ax[2].imshow(color_img)\n",
        "    ax[2].set_title('Ground Truth')\n",
        "    plt.show()\n",
        "\n",
        "def print_gpu_mem():\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"Allocated: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
        "        print(f\"Reserved: {torch.cuda.memory_reserved()/1024**2:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ed64558",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ed64558",
        "outputId": "780a24a1-0c83-473a-ea60-798b8854ac68"
      },
      "outputs": [],
      "source": [
        "lpips_fn = lpips.LPIPS(net='alex').to(device)\n",
        "fid_metric = FrechetInceptionDistance(feature=2048).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f0c2085",
      "metadata": {
        "id": "1f0c2085"
      },
      "outputs": [],
      "source": [
        "# Metric Functions\n",
        "def compute_metrics(pred, target):\n",
        "    pred_np = pred.detach().cpu().numpy()\n",
        "    target_np = target.detach().cpu().numpy()\n",
        "    psnr_batch, ssim_batch = [], []\n",
        "    for i in range(pred_np.shape[0]):\n",
        "        pred_img = np.transpose(pred_np[i], (1, 2, 0))\n",
        "        target_img = np.transpose(target_np[i], (1, 2, 0))\n",
        "        psnr_batch.append(psnr(target_img, pred_img, data_range=1.0))\n",
        "        ssim_batch.append(ssim(target_img, pred_img, channel_axis=-1, data_range=1.0))\n",
        "    return np.mean(psnr_batch), np.mean(ssim_batch)\n",
        "\n",
        "# Visualization\n",
        "def show_images(gray, pred, color, idx, title=\"\"):\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(9, 3))\n",
        "    axs[0].imshow(gray[idx].squeeze().cpu().numpy(), cmap='gray')\n",
        "    axs[0].set_title(f\"Grayscale {title}\")\n",
        "    axs[1].imshow(pred[idx].permute(1, 2, 0).detach().cpu().numpy())\n",
        "    axs[1].set_title(f\"Predicted Color {title}\")\n",
        "    axs[2].imshow(color[idx].permute(1, 2, 0).cpu().numpy())\n",
        "    axs[2].set_title(f\"Ground Truth {title}\")\n",
        "    for ax in axs:\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Save Checkpoint\n",
        "def save_checkpoint(model, optimizer, epoch, path='best_model.pth'):\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'epoch': epoch\n",
        "    }, path)\n",
        "    files.download(path)\n",
        "\n",
        "def get_timestep_embedding(timesteps, embedding_dim: int):\n",
        "    \"\"\"\n",
        "    Modified to automatically handle device placement\n",
        "    \"\"\"\n",
        "    device = timesteps.device  # Get device from input tensor\n",
        "    half_dim = embedding_dim // 2\n",
        "    emb = math.log(10000) / (half_dim - 1)\n",
        "    emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
        "    emb = timesteps.float()[:, None] * emb[None, :]\n",
        "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
        "\n",
        "    if embedding_dim % 2 == 1:\n",
        "        emb = F.pad(emb, (0, 1, 0, 0))\n",
        "\n",
        "    return emb\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03bfba67",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "03bfba67",
        "outputId": "3d1cafdb-5a99-46ae-f5ab-83a825bb7ee1"
      },
      "outputs": [],
      "source": [
        "# Initialize components\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = TimestepUNet(base_channels=48, time_dim=64).to(device)\n",
        "noise_scheduler = CosineNoiseScheduler(num_timesteps=1000, device=device)\n",
        "criterion = CombinedLoss(lpips_weight=0.5, deltae_weight=2.0, mse_weight=0.1).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Start training\n",
        "train_diffusion_model(\n",
        "    model=model,\n",
        "    noise_scheduler=noise_scheduler,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    num_epochs=20,\n",
        "    device=device,\n",
        "    accumulation_steps=4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed00543f",
      "metadata": {
        "id": "ed00543f"
      },
      "outputs": [],
      "source": [
        "# Save final model\n",
        "final_path = \"/content/final_model.pth\"\n",
        "save_checkpoint(model, optimizer, 20, path=final_path)\n",
        "sys.stdout.write(\"Final model saved!\\n\")\n",
        "\n",
        "# Download to local machine\n",
        "from google.colab import files\n",
        "files.download(final_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "azP-CwOLW5QI",
      "metadata": {
        "id": "azP-CwOLW5QI"
      },
      "outputs": [],
      "source": [
        "def train_dual_diffusion_models(model_noise, model_rgb, noise_scheduler,\n",
        "                                criterion_noise, criterion_rgb,\n",
        "                                optimizer_noise, optimizer_rgb,\n",
        "                                num_epochs, device, accumulation_steps=4):\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    best_val_loss_noise = float('inf')\n",
        "    best_val_loss_rgb = float('inf')\n",
        "\n",
        "    train_loader = get_cifar10_loader(batch_size=64)\n",
        "    val_loader = get_cifar10_loader(batch_size=64, train=False)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        sys.stdout.write(f\"\\nStarting epoch {epoch+1}\\n\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        model_noise.train()\n",
        "        model_rgb.train()\n",
        "\n",
        "        running_loss_noise = 0.0\n",
        "        running_loss_rgb = 0.0\n",
        "        running_psnr = 0.0\n",
        "        running_ssim = 0.0\n",
        "        running_lpips = 0.0\n",
        "\n",
        "        optimizer_noise.zero_grad()\n",
        "        optimizer_rgb.zero_grad()\n",
        "\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            gray = batch['gray'].to(device, non_blocking=True)\n",
        "            color = batch['color'].to(device, non_blocking=True)\n",
        "\n",
        "            if i % 1000 == 0:\n",
        "                print_gpu_mem()\n",
        "\n",
        "            t = torch.randint(0, noise_scheduler.num_timesteps, (color.size(0),), device=device)\n",
        "            noise = torch.randn_like(color)\n",
        "            noised_color = noise_scheduler.add_noise(color, noise, t)\n",
        "\n",
        "            with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
        "                pred_noise = model_noise(noised_color, gray, t)\n",
        "                loss_noise = criterion_noise(pred_noise, noise) / accumulation_steps\n",
        "\n",
        "                pred_noise_rgb = model_rgb(noised_color, gray, t)\n",
        "                denoised_rgb = noise_scheduler.remove_noise(noised_color, pred_noise_rgb, t)\n",
        "                loss_rgb = criterion_rgb(denoised_rgb, color) / accumulation_steps\n",
        "\n",
        "            scaler.scale(loss_noise).backward()\n",
        "            scaler.scale(loss_rgb).backward()\n",
        "\n",
        "            if (i + 1) % accumulation_steps == 0:\n",
        "                scaler.step(optimizer_noise)\n",
        "                scaler.step(optimizer_rgb)\n",
        "                scaler.update()\n",
        "                optimizer_noise.zero_grad()\n",
        "                optimizer_rgb.zero_grad()\n",
        "\n",
        "            running_loss_noise += loss_noise.item() * accumulation_steps\n",
        "            running_loss_rgb += loss_rgb.item() * accumulation_steps\n",
        "\n",
        "            with torch.no_grad():\n",
        "                denoised = noise_scheduler.remove_noise(noised_color, pred_noise, t)\n",
        "                batch_psnr, batch_ssim = compute_metrics(denoised, color)\n",
        "                batch_lpips = lpips_fn(denoised, color).mean().item()\n",
        "\n",
        "            running_psnr += batch_psnr\n",
        "            running_ssim += batch_ssim\n",
        "            running_lpips += batch_lpips\n",
        "\n",
        "            sys.stdout.write(\n",
        "                f\"\\r[Epoch {epoch+1}/{num_epochs}] [Batch {i+1}/{len(train_loader)}] \"\n",
        "                f\"LossN: {loss_noise.item() * accumulation_steps:.4f} \"\n",
        "                f\"LossRGB: {loss_rgb.item() * accumulation_steps:.4f} \"\n",
        "                f\"PSNR: {batch_psnr:.2f} SSIM: {batch_ssim:.4f} LPIPS: {batch_lpips:.4f}\"\n",
        "            )\n",
        "            sys.stdout.flush()\n",
        "\n",
        "            if (i + 1) % 100 == 0:\n",
        "                idx = random.randint(0, gray.size(0) - 1)\n",
        "                show_images(gray, noise_scheduler.remove_noise(noised_color, pred_noise, t), color, idx, title=\"Noise Model\")\n",
        "                show_images(gray, noise_scheduler.remove_noise(noised_color, pred_noise_rgb, t), color, idx, title=\"RGB Model\")\n",
        "\n",
        "        avg_loss_noise = running_loss_noise / len(train_loader)\n",
        "        avg_loss_rgb = running_loss_rgb / len(train_loader)\n",
        "        avg_psnr = running_psnr / len(train_loader)\n",
        "        avg_ssim = running_ssim / len(train_loader)\n",
        "        avg_lpips = running_lpips / len(train_loader)\n",
        "\n",
        "        sys.stdout.write('\\n')\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # Validation\n",
        "        model_noise.eval()\n",
        "        model_rgb.eval()\n",
        "        val_loss_noise = 0.0\n",
        "        val_loss_rgb = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, batch in enumerate(val_loader):\n",
        "                gray = batch['gray'].to(device)\n",
        "                color = batch['color'].to(device)\n",
        "                t = torch.full((color.size(0),), noise_scheduler.num_timesteps//2, device=device)\n",
        "                noise = torch.randn_like(color)\n",
        "                noised_color = noise_scheduler.add_noise(color, noise, t)\n",
        "\n",
        "                pred_noise = model_noise(noised_color, gray, t)\n",
        "                val_loss_noise += criterion_noise(pred_noise, noise).item()\n",
        "                denoised_noise = noise_scheduler.remove_noise(noised_color, pred_noise, t)\n",
        "\n",
        "                pred_noise_rgb = model_rgb(noised_color, gray, t)\n",
        "                denoised_rgb = noise_scheduler.remove_noise(noised_color, pred_noise_rgb, t)\n",
        "                val_loss_rgb += criterion_rgb(denoised_rgb, color).item()\n",
        "\n",
        "                if i == 0:\n",
        "                    show_images(gray, denoised_noise, color, idx=0, title=\"Val Noise Model\")\n",
        "                    show_images(gray, denoised_rgb, color, idx=0, title=\"Val RGB Model\")\n",
        "\n",
        "        val_loss_noise /= len(val_loader)\n",
        "        val_loss_rgb /= len(val_loader)\n",
        "        print(f\"[Epoch {epoch+1}] Val Loss (Noise): {val_loss_noise:.4f} | Val Loss (RGB): {val_loss_rgb:.4f}\")\n",
        "\n",
        "        if val_loss_noise < best_val_loss_noise:\n",
        "            best_val_loss_noise = val_loss_noise\n",
        "            save_checkpoint(model_noise, optimizer_noise, epoch, path=\"best_model_noise.pth\")\n",
        "            print(\"Saved best Noise Model\")\n",
        "\n",
        "        if val_loss_rgb < best_val_loss_rgb:\n",
        "            best_val_loss_rgb = val_loss_rgb\n",
        "            save_checkpoint(model_rgb, optimizer_rgb, epoch, path=\"best_model_rgb.pth\")\n",
        "            print(\"Saved best RGB Model\")\n",
        "\n",
        "        torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HUUXyh6kXQlE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HUUXyh6kXQlE",
        "outputId": "8d89029b-9af6-4875-9bf2-ebf060dc9810"
      },
      "outputs": [],
      "source": [
        "model_noise = TimestepUNet(base_channels=48, time_dim=64).to(device)\n",
        "model_rgb = TimestepUNet(base_channels=48, time_dim=64).to(device)\n",
        "\n",
        "noise_scheduler = CosineNoiseScheduler(num_timesteps=1000, device=device)\n",
        "\n",
        "# LPIPS and DeltaE are used by both criterions\n",
        "lpips_fn = lpips.LPIPS(net='alex').to(device).eval()\n",
        "\n",
        "# Criterion for noise prediction\n",
        "criterion_noise = CombinedLoss(lpips_weight=0.0, deltae_weight=0.0, mse_weight=1.0).to(device)\n",
        "\n",
        "# Criterion for RGB prediction\n",
        "criterion_rgb = CombinedLoss(lpips_weight=0.5, deltae_weight=2.0, mse_weight=0.1).to(device)\n",
        "\n",
        "optimizer_noise = torch.optim.AdamW(model_noise.parameters(), lr=1e-4)\n",
        "optimizer_rgb = torch.optim.AdamW(model_rgb.parameters(), lr=1e-4)\n",
        "\n",
        "# Start dual-model training\n",
        "train_dual_diffusion_models(\n",
        "    model_noise=model_noise,\n",
        "    model_rgb=model_rgb,\n",
        "    noise_scheduler=noise_scheduler,\n",
        "    criterion_noise=criterion_noise,\n",
        "    criterion_rgb=criterion_rgb,\n",
        "    optimizer_noise=optimizer_noise,\n",
        "    optimizer_rgb=optimizer_rgb,\n",
        "    num_epochs=20,\n",
        "    device=device,\n",
        "    accumulation_steps=4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9JwJeiQ1YXsm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JwJeiQ1YXsm",
        "outputId": "de0b5009-f385-41cd-db0d-54a0e820505d"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "def save_generated_images(model, loader, device, save_dir, noise_scheduler, t_value=500):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    model.eval()\n",
        "    image_count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc=f\"Generating images for {save_dir}\"):\n",
        "            gray = batch['gray'].to(device)\n",
        "            color = batch['color'].to(device)\n",
        "            noise = torch.randn_like(color)\n",
        "            t = torch.full((color.size(0),), t_value, device=device)\n",
        "\n",
        "            noised_color = noise_scheduler.add_noise(color, noise, t)\n",
        "            pred_noise = model(noised_color, gray, t)\n",
        "            pred_color = noise_scheduler.remove_noise(noised_color, pred_noise, t)\n",
        "\n",
        "            for img in pred_color:\n",
        "                save_image(img, f\"{save_dir}/{image_count:06}.png\")\n",
        "                image_count += 1\n",
        "\n",
        "def validate_models(model_noise, model_rgb, val_loader, device):\n",
        "    print(\"Saving generated images...\")\n",
        "    save_generated_images(model_noise, val_loader, device, \"/content/gen_noise\", noise_scheduler)\n",
        "    save_generated_images(model_rgb, val_loader, device, \"/content/gen_rgb\", noise_scheduler)\n",
        "\n",
        "    print(\"\\nCalculating FID...\")\n",
        "    fid_noise = calculate_metrics(\n",
        "        input1=\"/content/gen_noise\",\n",
        "        input2=\"/content/real_val\",\n",
        "        cuda=torch.cuda.is_available(),\n",
        "        isc=False, fid=True, kid=True, ppl=False,\n",
        "        verbose=False\n",
        "    )\n",
        "    fid_rgb = calculate_metrics(\n",
        "        input1=\"/content/gen_rgb\",\n",
        "        input2=\"/content/real_val\",\n",
        "        cuda=torch.cuda.is_available(),\n",
        "        isc=False, fid=True, kid=True, ppl=False,\n",
        "        verbose=False\n",
        "    )\n",
        "    print(\"Noise Model FID:\", fid_noise['frechet_inception_distance'])\n",
        "    print(\"RGB Model FID:\", fid_rgb['frechet_inception_distance'])\n",
        "    print(\"Noise Model KID:\", fid_noise['kernel_inception_distance_mean'])\n",
        "    print(\"RGB Model KID:\", fid_rgb['kernel_inception_distance_mean'])\n",
        "\n",
        "    print(\"\\nCalculating perceptual and pixel metrics...\")\n",
        "    lpips_fn = lpips.LPIPS(net='alex').to(device).eval()\n",
        "    criterion = CombinedLoss().to(device)\n",
        "\n",
        "    def calculate_other_metrics(model):\n",
        "        model.eval()\n",
        "        lpips_vals, psnr_vals, ssim_vals, deltae_vals = [], [], [], []\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc=\"Metric Evaluation\"):\n",
        "                gray = batch['gray'].to(device)\n",
        "                color = batch['color'].to(device)\n",
        "                t = torch.full((color.size(0),), 500, device=device)\n",
        "                noise = torch.randn_like(color)\n",
        "                noised_color = noise_scheduler.add_noise(color, noise, t)\n",
        "                pred_noise = model(noised_color, gray, t)\n",
        "                pred_color = noise_scheduler.remove_noise(noised_color, pred_noise, t)\n",
        "\n",
        "                lpips_vals.extend(lpips_fn(pred_color, color).cpu().numpy())\n",
        "                psnr, ssim = compute_metrics(pred_color, color)\n",
        "                psnr_vals.append(psnr)\n",
        "                ssim_vals.append(ssim)\n",
        "                deltae = criterion.batch_delta_e_loss(pred_color, color).cpu().numpy()\n",
        "                deltae_vals.append(deltae)\n",
        "\n",
        "        return {\n",
        "            'LPIPS': np.mean(lpips_vals),\n",
        "            'PSNR': np.mean(psnr_vals),\n",
        "            'SSIM': np.mean(ssim_vals),\n",
        "            'DeltaE': np.mean(deltae_vals),\n",
        "        }\n",
        "\n",
        "    metrics_noise = calculate_other_metrics(model_noise)\n",
        "    metrics_rgb = calculate_other_metrics(model_rgb)\n",
        "\n",
        "    print(\"\\n--- Additional Metrics ---\")\n",
        "    for k, v in metrics_noise.items():\n",
        "        print(f\"Noise Model {k}: {v:.4f}\")\n",
        "    for k, v in metrics_rgb.items():\n",
        "        print(f\"RGB Model {k}: {v:.4f}\")\n",
        "\n",
        "\n",
        "model_noise = TimestepUNet(base_channels=48, time_dim=64).to(device)\n",
        "model_rgb = TimestepUNet(base_channels=48, time_dim=64).to(device)\n",
        "\n",
        "model_noise.load_state_dict(torch.load(\"/content/best_model_noise.pth\")['model_state_dict'])\n",
        "model_rgb.load_state_dict(torch.load(\"/content/best_model_rgb.pth\")['model_state_dict'])\n",
        "\n",
        "val_loader = get_cifar10_loader(batch_size=64, train=False)\n",
        "save_generated_images(model_rgb, val_loader, device, \"/content/real_val\", noise_scheduler=CosineNoiseScheduler())\n",
        "\n",
        "validate_models(model_noise, model_rgb, val_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AblUicVvNqV-",
      "metadata": {
        "id": "AblUicVvNqV-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
